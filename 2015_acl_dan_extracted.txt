Deep Unordered Composition Rivals Syntactic Methods
for Text Classification
Mohit Iyyer,
1
Varun Manjunatha,
1
Jordan Boyd-Graber,
2
Hal Daum
 ́
e III
1
1
University of Maryland, Department of Computer Science andUMIACS
2
University of Colorado, Department of Computer Science
{miyyer,varunm,hal}@umiacs.umd.edu,Jordan.Boyd.Graber@colorado.edu
Abstract
Many  existing  deep  learning  models  for
natural language processing tasks focus on
learning thecompositionalityof their in-
puts, which requires many expensive com-
putations. We present a simple deep neural
network that competes with and, in some
cases,  outperforms  such  models  on  sen-
timent  analysis  and  factoid  question  an-
swering tasks while taking only a fraction
of the training time.  While our model is
syntactically-ignorant, we show significant
improvements over previous bag-of-words
models by deepening our network and ap-
plying a novel variant of dropout.  More-
over, our model performs better than syn-
tactic models on datasets with high syn-
tactic variance.  We show that our model
makes similar errors to syntactically-aware
models, indicating that for the tasks we con-
sider, nonlinearly transforming the input is
more important than tailoring a network to
incorporate word order and syntax.
1    Introduction
Vector space models for natural language process-
ing (NLP) represent words using low dimensional
vectors called embeddings. To apply vector space
models to sentences or documents, one must first
select an appropriatecomposition function, which
is a mathematical process for combining multiple
words into a single vector.
Composition functions fall into two classes:un-
orderedandsyntactic. Unordered functions treat in-
put texts as bags of word embeddings, while syntac-
tic functions take word order and sentence structure
into account.  Previously published experimental
results have shown that syntactic functions outper-
form unordered functions on many tasks (Socher
et al., 2013b; Kalchbrenner and Blunsom, 2013).
However, there is a tradeoff: syntactic functions
require more training time than unordered compo-
sition functions and are prohibitively expensive in
the case of huge datasets or limited computing re-
sources. For example, the recursive neural network
(Section 2) computes costly matrix/tensor products
and nonlinearities at every node of a syntactic parse
tree, which limits it to smaller datasets that can be
reliably parsed.
We introduce a deep unordered model that ob-
tains near state-of-the-art accuracies on a variety of
sentence and document-level tasks with just min-
utes of training time on an average laptop computer.
This model,  the deep averaging network (DAN),
works in three simple steps:
1.take  the  vector  average  of  the  embeddings
associated with an input sequence of tokens
2.
pass that average through one or more feed-
forward layers
3.perform  (linear)  classification  on  the  final
layer's representation
The model can be improved by applying a novel
dropout-inspired regularizer: for each training in-
stance, randomly drop some of the tokens' embed-
dings before computing the average.
We evaluateDANs on sentiment analysis and fac-
toid question answering tasks at both the sentence
and document level in Section 4. Our model's suc-
cesses demonstrate that for these tasks, the choice
of composition function is not as important as ini-
tializing with pretrained embeddings and using a
deep network.  Furthermore,DANs, unlike more
complex composition functions, can be effectively
trained on data that have high syntactic variance. A

qualitative analysis of the learned layers suggests
that the model works by magnifying tiny but mean-
ingful differences in the vector average through
multiple hidden layers, and a detailed error analy-
sis shows that syntactically-aware models actually
make very similar errors to those of the more na
 ̈
ıve
DAN.
2    Unordered vs. Syntactic Composition
Our goal is to marry the speed of unordered func-
tions  with  the  accuracy  of  syntactic  functions.
In  this  section,  we  first  describe  a  class  of  un-
ordered composition functions dubbed "neural bag-
of-words models" (NBOW). We then explore more
complex  syntactic  functions  designed  to  avoid
many of the pitfalls associated withNBOWmod-
els. Finally, we present the deep averaging network
(DAN), which stacks nonlinear layers over the tradi-
tionalNBOWmodel and achieves performance on
par with or better than that of syntactic functions.
2.1    Neural Bag-of-Words Models
For simplicity, consider text classification: map an
input sequence of tokensXto one ofklabels. We
first apply a composition functiongto the sequence
of word embeddingsv
w
forw∈X. The output of
this composition function is a vectorzthat serves
as input to a logistic regression function.
In our instantiation ofNBOW,gaverages word
embeddings
1
z=g(w∈X) =
1
|X|
∑
w∈X
v
w
.(1)
Feedingzto a softmax layer induces estimated
probabilities for each output label
ˆy=softmax(W
s
·z+b),(2)
where the softmax function is
softmax(q) =
expq
∑
k
j=1
expq
j
(3)
W
s
is ak×dmatrix for a dataset withkoutput
labels, andbis a bias term.
We train theNBOWmodel to minimize cross-
entropy error, which for a single training instance
with ground-truth labelyis
`(ˆy) =
k
∑
p=1
y
p
log(ˆy
p
).(4)
1
Preliminary experiments indicate that averaging outper-
forms the vector sum used inNBOWfrom Kalchbrenner et al.
(2014).
Before we describe our deep extension of the
NBOWmodel, we take a quick detour to discuss
syntactic composition functions.  Connections to
other representation frameworks are discussed fur-
ther in Section 4.
2.2    Considering Syntax for Composition
Given a sentence like "You'll be more entertained
getting  hit  by  a  bus",  an  unordered  model  like
NBOWmight be deceived by the word "entertained"
to return a positive prediction.  In contrast,  syn-
tactic composition functions rely on the order and
structure of the input to learn how one word or
phrase affects another, sacrificing computational
efficiency in the process.  In subsequent sections,
we argue that this complexity is not matched by a
corresponding gain in performance.
Recursive neural networks (RecNNs) are syntac-
tic functions that rely on natural language's inher-
ent structure to achieve state-of-the-art accuracies
on sentiment analysis tasks (Tai et al., 2015). As in
NBOW, each word type has an associated embed-
ding.  However, the composition functiongnow
depends on aparse treeof the input sequence. The
representation for any internal node in a binary
parse tree is computed as a nonlinear function of
the representations of its children (Figure 1, left).
A more powerfulRecNNvariant is the recursive
neural tensor network (RecNTN), which modifies
gto include a costly tensor product (Socher et al.,
2013b).
WhileRecNNs  can  model  complex  linguistic
phenomena like negation (Hermann et al., 2013),
they require much more training time thanNBOW
models. The nonlinearities and matrix/tensor prod-
ucts  at  each  node  of  the  parse  tree  are  expen-
sive, especially as model dimensionality increases.
RecNNs also require an error signal ateverynode.
One  root  softmax  is  not  strong  enough  for  the
model to learn compositional relations and leads
to worse accuracies than standard bag-of-words
models (Li, 2014).  Finally,RecNNs require rela-
tively consistent syntax between training and test
data due to their reliance on parse trees and thus
cannot effectively incorporate out-of-domain data,
as we show in our question-answering experiments.
Kim (2014) shows that some of these issues can
be avoided by using a convolutional network in-
stead of aRecNN, but the computational complex-
ity increases even further (see Section 4 for runtime
comparisons).
What contributes most to the power of syntactic

Predator
c
1
is
c
2
a
c
3
masterpiece
c
4
z
1
=f(W
[
c
3
c
4
]
+b)
z
2
=f(W
[
c
2
z
1
]
+b)
z
3
=f(W
[
c
1
z
2
]
+b)
softmax
softmax
softmax
RecNN
Predator
c
1
is
c
2
a
c
3
masterpiece
c
4
av=
4
∑
i=1
c
i
4
h
1
=f(W
1
·av+b
1
)
h
2
=f(W
2
·h
1
+b
2
)
softmax
DAN
Figure 1: On the left, aRecNNis given an input sentence for sentiment classification.  Softmax layers
are placed above every internal node to avoid vanishing gradient issues. On the right is a two-layerDAN
taking the same input. While theRecNNhas to compute a nonlinear representation (purple vectors) for
every node in the parse tree of its input, thisDANonly computes two nonlinear layers for every possible
input.
functions: the compositionality or the nonlineari-
ties? Socher et al. (2013b) report that removing the
nonlinearities from theirRecNNmodels drops per-
formance on the Stanford Sentiment Treebank by
over 5% absolute accuracy. Most unordered func-
tions are linear mappings between bag-of-words
features and output labels,  so might they suffer
from the same issue? To isolate the effects of syn-
tactic composition from the nonlinear transforma-
tions that are crucial toRecNNperformance, we
investigate how well a deep version of theNBOW
model performs on tasks that have recently been
dominated by syntactically-aware models.
3    Deep Averaging Networks
The intuition behind deep feed-forward neural net-
works is that each layer learns a more abstract rep-
resentation of the input than the previous one (Ben-
gio et al., 2013). We can apply this concept to the
NBOWmodel discussed in Section 2.1 with the ex-
pectation that each layer will increasingly magnify
small but meaningful differences in the word em-
bedding average. To be more concrete, takes
1
as
the sentence "I really loved Rosamund Pike's per-
formance in the movie Gone Girl" and generates
2
ands
3
by replacing "loved" with "liked" and then
again by "despised". The vector averages of these
three sentences are almost identical, but the aver-
ages associated with the synonymous sentencess
1
ands
2
are slightly more similar to each other than
they are tos
3
's average.
Could adding depth toNBOWmake small such
distinctions as this one more apparent?  In Equa-
tion 1, we computez, the vector representation for
input textX, by averaging the word vectorsv
w∈X
.
Instead of directly passing this representation to an
output layer, we can further transformzby adding
more layers before applying the softmax. Suppose
we havenlayers,z
1...n
. We compute each layer
z
i
=g(z
i−1
) =f(W
i
·z
i−1
+b
i
)(5)
and feed the final layer's representation,z
n
, to a
softmax layer for prediction (Figure 1, right).
This model, which we call a deep averaging net-
work (DAN), is still unordered, but its depth allows
it to capture subtle variations in the input better
than the standardNBOWmodel. Furthermore, com-
puting each layer requires just a single matrix multi-
plication, so the complexity scales with the number
of layers rather than the number of nodes in a parse
tree. In practice, we find no significant difference
between the training time of aDANand that of the
shallowNBOWmodel.
3.1    Word Dropout Improves Robustness
Dropout regularizes neural networks by randomly
setting hidden and/or input units to zero with some
probabilityp(Hinton et al.,  2012;  Srivastava et
al., 2014).  Given a neural network withnunits,
dropout prevents overfitting by creating an ensem-
ble of2
n
different networks that share parameters,
where each network consists of some combination
of dropped and undropped units. Instead of drop-
ping units, a natural extension for theDANmodel is
to randomly drop word tokens' entireword embed-
dingsfrom the vector average. Using this method,

which we callword dropout, our network theoreti-
cally sees2
|X|
different token sequences for each
inputX.
We  posit  a  vectorrwith|X|independent
Bernoulli trials, each of which equals 1 with prob-
abilityp. The embeddingv
w
for tokenwinXis
dropped from the average ifr
w
is 0, which expo-
nentially increases the number of unique examples
the network sees during training. This allows us to
modify Equation 1:
r
w
∼Bernoulli(p)(6)
ˆ
X={w|w∈Xandr
w
>0}(7)
z=g(w∈X) =
∑
w∈
ˆ
X
v
w
|
ˆ
X|
.(8)
Depending  on  the  choice  ofp,  many  of  the
"dropped" versions of an original training instance
will be very similar to each other, but for shorter
inputs this is less likely.   We might drop a very
important token, such as "horrible" in "the crab
rangoon was especially horrible"; however, since
the number of word types that are predictive of the
output labels is low compared to non-predictive
ones (e.g., neutral words in sentiment analysis), we
always see improvements using this technique.
Theoretically, word dropout can also be applied
to other neural network-based approaches.  How-
ever, we observe no significant performance differ-
ences in preliminary experiments when applying
word dropout to leaf nodes inRecNNs for senti-
ment analysis (dropped leaf representations are set
to zero vectors), and it slightly hurts performance
on the question answering task.
4    Experiments
We  compareDANs  to  both  the  shallowNBOW
model as well as more complicated syntactic mod-
els on sentence and document-level sentiment anal-
ysis and factoid question answering tasks. TheDAN
architecture we use for each task is almost identi-
cal, differing across tasks only in the type of output
layer and the choice of activation function.  Our
results show thatDANs outperform other bag-of-
words models and many syntactic models with very
little training time.
2
On the question-answering
task,DANs effectively train on out-of-domain data,
whileRecNNs struggle to reconcile the syntactic
differences between the training and test data.
2
Code athttp://github.com/miyyer/dan.
ModelRTSSTSSTIMDBTime
finebin(s)
DAN-ROOT—46.985.7—31
DAN-RAND77.345.483.288.8136
DAN80.347.786.389.4136
NBOW-RAND76.242.381.488.991
NBOW79.043.683.689.091
BiNB—41.983.1——
NBSVM-bi79.4——91.2—
RecNN
∗
77.743.282.4——
RecNTN
∗
—45.785.4——
DRecNN—49.886.6—431
TreeLSTM—50.686.9——
DCNN
∗
—48.586.989.4—
PVEC
∗
—48.787.892.6—
CNN-MC81.147.488.1—2,452
WRRBM
∗
———89.2—
Table 1:DANs achieve comparable sentiment accu-
racies to syntactic functions (bottom third of table)
but require much less training time (measured as
time of a single epoch on theSSTfine-grained task).
Asterisked models are initialized either with differ-
ent pretrained embeddings or randomly.
4.1    Sentiment Analysis
Recently,  syntactic  composition  functions  have
revolutionized both fine-grained and binary (pos-
itive or negative) sentiment analysis. We conduct
sentence-level sentiment experiments on the Rot-
ten Tomatoes (RT) movie reviews dataset (Pang
and Lee, 2005) and its extension with phrase-level
labels, the Stanford Sentiment Treebank (SST) in-
troduced by Socher et al. (2013b).  Our model is
also effective on the document-levelIMDBmovie
review dataset of Maas et al. (2011).
4.1.1    Neural Baselines
Most neural approaches to sentiment analysis are
variants of either recursive or convolutional net-
works.   Our  recursive  neural  network  baselines
include standardRecNNs (Socher et al.,  2011b),
RecNTNs, the deep recursive network (DRecNN)
proposed  by
 ̇
Irsoy  and  Cardie  (2014),  and  the
TREE-LSTMof  (Tai  et  al.,  2015).Convolu-
tional network baselines include the dynamic con-
volutional  network  (Kalchbrenner  et  al.,  2014,
DCNN) and the convolutional neural network multi-
channel (Kim,  2014,CNN-MC).   Our other neu-
ral baselines are the sliding-window based para-
graph vector (Le and Mikolov, 2014,PVEC)
3
and
3
PVECis computationally expensive at both training and
test time and requires enough memory to store a vector for
every paragraph in the training data.

the word-representation restricted Boltzmann ma-
chine  (Dahl  et  al.,  2012,WRRBM),  which  only
works on the document-levelIMDBtask.
4
4.1.2    Non-Neural Baselines
We also compare to non-neural baselines, specif-
ically the bigram na
 ̈
ıve Bayes (BINB) and na
 ̈
ıve
Bayes support vector machine (NBSVM-BI) mod-
els introduced by Wang and Manning (2012), both
of which are memory-intensive due to huge feature
spaces of size|V|
2
.
4.1.3    DAN Configurations
In Table 1, we compare a variety ofDANandNBOW
configurations
5
to the baselines described above. In
particular, we are interested in not only comparing
DANaccuracies to those of the baselines, but also
how initializing with pretrained embeddings and re-
stricting the model to only root-level labels affects
performance. With this in mind, theNBOW-RAND
andDAN-RANDmodels are initialized with ran-
dom 300-dimensional word embeddings, while the
other models are initialized with publicly-available
300-dGloVevectors trained over the Common
Crawl (Pennington et al., 2014). TheDAN-ROOT
model only has access to sentence-level labels for
SSTexperiments, while all other models are trained
on labeled phrases (if they exist) in addition to sen-
tences. We train allNBOWandDANmodels using
AdaGrad (Duchi et al., 2011).
We applyDANs to documents by averaging the
embeddings  for  all  of  a  document's  tokens  and
then feeding that average through multiple layers
as before. Since the representations computed by
DANs are alwaysd-dimensional vectors regardless
of the input size, they are efficient with respect to
both memory and computational cost. We find that
the hyperparameters selected on theSSTalso work
well for theIMDBtask.
4.1.4    Dataset Details
We  evaluate  over  both  fine-grained  and  binary
sentence-level classification tasks on theSST, and
just the binary task onRTandIMDB.  In the fine-
grainedSSTsetting, each sentence has a label from
zero to five where two is the neutral class. For the
binary task, we ignore all neutral sentences.
6
4
TheWRRBMis trained using a slow Metropolis-Hastings
algorithm.
5
Best hyperparameters chosen by cross-validation: three
300-d ReLu layers, word dropout probabilityp= 0.3, L2
regularization weight of 1e-5 applied to all parameters
6
Our fine-grainedSSTsplit is{train:  8,544, dev:  1,101,
test: 2,210}, while our binary split is{train: 6,920, dev:872,
4.1.5    Results
TheDANachieves the second best reported result
on  theRTdataset,  behind  only  the  significantly
slowerCNN-MCmodel. It's also competitive with
more complex models on theSSTand outperforms
theDCNNandWRRBMon  the  document-level
IMDBtask.  Interestingly, theDANachieves good
performance on theSSTwhen trained with only
sentence-level labels,  indicating that it does not
suffer from the vanishing error signal problem that
plaguesRecNNs. Since acquiring labelled phrases
is often expensive (Sayeed et al., 2012; Iyyer et
al.,  2014b),  this result is promising for large or
messy datasets where fine-grained annotation is
infeasible.
4.1.6    Timing Experiments
DANs require less time per epoch and—in general—
require  fewer  epochs  than  their  syntactic  coun-
terparts.   We  compareDANruntime  on  theSST
to publicly-available implementations of syntactic
baselines in the last column of Table 1; the reported
times are for a single epoch to control for hyper-
parameter choices such as learning rate,  and all
models use 300-dword vectors.  Training aDAN
on just sentence-level labels on theSSTtakes under
five minutes on a single core of a laptop;  when
labeled phrases are added as separate training in-
stances, training time jumps to twenty minutes.
7
All timing experiments were performed on a single
core of an Intel I7 processor with 8GB of RAM.
4.2    Factoid Question Answering
DANs work well for sentiment analysis, but how
do they do on otherNLPtasks?   We shift gears
to a paragraph-length factoid question answering
task  and  find  that  our  model  outperforms  other
unordered functions as well as a more complex
syntacticRecNNmodel.   More interestingly,  we
find that unlike theRecNN, theDANsignificantly
benefits  from  out-of-domain  Wikipedia  training
data.
Quiz bowl is a trivia competition in which play-
ers are asked four-to-six sentence questions about
entities (e.g., authors, battles, or events).  It is an
ideal task to evaluateDANs because there is prior
test:1,821}.  Split sizes increase by an order of magnitude
when labeled phrases are added to the training set.  ForRT,
we do 10-fold CV over a balanced binary dataset of 10,662
sentences.  Similarly, for theIMDBexperiments we use the
provided balanced binary training set of 25,000 documents.
7
We also find thatDANs take significantly fewer epochs to
reach convergence than syntactic models.

ModelPos 1Pos 2FullTime(s)
BoW-DT35.457.760.2—
IR37.565.971.4N/A
QANTA47.172.173.7314
DAN46.470.871.818
IR-WIKI53.776.677.5N/A
QANTA-WIKI46.572.873.91,648
DAN-WIKI54.875.577.1119
Table 2:  TheDANachieves slightly lower accu-
racies  than  the  more  complexQANTAin  much
less  training  time,  even  at  early  sentence  posi-
tions where compositionality plays a bigger role.
When Wikipedia is added to the training set (bot-
tom half of table), theDANoutperformsQANTA
and achieves comparable accuracy to a state-of-the-
art information retrieval baseline, which highlights
a benefit of ignoring word order for this task.
l
l
l
l
l
l
69
70
71
0.00.10.20.30.40.5
Dropout Probability
History QB Accuracy
Effect of Word Dropout
Figure 2:  Randomly dropping out 30% of words
from the vector average is optimal for the quiz bowl
task, yielding a gain in absolute accuracy of almost
3% on the quiz bowl question dataset compared to
the same model trained with no word dropout.
work using both syntactic and unordered models
for quiz bowl question answering. In Boyd-Graber
et al. (2012),  na
 ̈
ıve Bayes bag-of-words models
(BOW-DT) and sequential language models work
well on easy questions but poorly on harder ones.
A dependency-treeRecNNcalledQANTAproposed
in Iyyer et al. (2014a) shows substantial improve-
ments, leading to the hypothesis that correctly mod-
eling compositionality is crucial for answering hard
questions.
4.2.1    Dataset and Experimental Setup
To test this, we train aDANover the history ques-
tions from Iyyer et al. (2014a).
8
This dataset is aug-
8
The training set contains 14,219 sentences over 3,761
questions.  For more detail about data and baseline systems,
mented with 49,581 sentence/page-title pairs from
the Wikipedia articles associated with the answers
in the dataset.  For fair comparison withQANTA,
we use a normalized tanh activation function at the
last layer instead of ReLu, and we also change the
output layer from a softmax to the margin rank-
ing loss (Weston et al., 2011) used inQANTA. We
initialize theDANwith the same pretrained 100-
d  word  embeddings  that  were  used  to  initialize
QANTA.
We  also  evaluate  the  effectiveness  of  word
dropout on this task in Figure 2. Cross-validation
indicates thatp= 0.3works best for question an-
swering, although the improvement in accuracy is
negligible for sentiment analysis. Finally, continu-
ing the trend observed in the sentiment experiments,
DANconverges much faster thanQANTA.
4.2.2    DANs Improve with Noisy Data
Table 2 shows that whileDANis slightly worse
thanQANTAwhen trained only on question-answer
pairs, it improves when trained on additional out-
of-domain Wikipedia data (DAN-WIKI), reaching
performance comparable to that of a state-of-the-art
information retrieval system (IR-WIKI).QANTA,
in contrast, barely improves when Wikipedia data is
added (QANTA-WIKI) possibly due to the syntactic
differences between Wikipedia text and quiz bowl
question text.
The most common syntactic structures in quiz
bowl sentences are imperative constructions such
as "Identify this British author who wrote Wuther-
ing  Heights",  which  are  almost  never  seen  in
Wikipedia. Furthermore, the subject of most quiz
bowl sentences is a pronoun or pronomial mention
referring to the answer, a property that is not true
of Wikipedia sentences (e.g.,  "Little of Emily's
work from this period survives, except for poems
spoken by characters."). Finally, many Wikipedia
sentences do not uniquely identify the title of the
page they come from, such as the following sen-
tence from Emily Bront
 ̈
e's page:  "She does not
seem to have made any friends outside her family."
While noisy data affect bothDANandQANTA, the
latter is further hampered by the syntactic diver-
gence between quiz bowl questions and Wikipedia,
which may explain the lack of improvement in ac-
curacy.
see Iyyer et al. (2014a).

0
10
20
30
40
50
012345
Layer
Perturbation Response
cool
okay
the worst
underwhelming 
Perturbation Response vs. Layer
Figure 3:  Perturbation response (difference in 1-
norm) at each layer of a 5-layerDANafter replac-
ingawesomeinthe film's performances were awe-
somewith four words of varying sentiment polarity.
While the shallowNBOWmodel does not show any
meaningful distinctions, we see that as the network
gets deeper,  negative sentences are increasingly
different from the original positive sentence.
l
l
l
ll
l
l
l
l
l
l
l
l
l
83
84
85
86
87
0246
Number of Layers
Binary Classification Accuracy
ll
ll
DAN
DAN−ROOT
Effect of Depth on Sentiment Accuracy
Figure 4:  Two to three layers is optimal for the
DANon theSSTbinary sentiment analysis task, but
adding any depth at all is an improvement over the
shallowNBOWmodel.
5    How Do DANs Work?
In this section we first examine how the deep layers
of theDANamplify tiny differences in the vector av-
erage that are predictive of the output labels. Next,
we compareDANs toDRecNNs on sentences that
contain negations and contrastive conjunctions and
find that both models make similar errors despite
the latter's increased complexity.  Finally, we an-
alyze the predictive ability of unsupervised word
embeddings on a simple sentiment task in an effort
to explain why initialization with these embeddings
improves theDAN.
5.1    Perturbation Analysis
Following the work of
 ̇
Irsoy and Cardie (2014), we
examine our network by measuring the response at
each hidden layer to perturbations in an input sen-
tence. In particular, we use the templatethe film's
performances  were  awesomeand  replace  the  fi-
nal word with increasingly negative polarity words
(cool,okay,underwhelming,the worst). For each
perturbed sentence, we observe how much the hid-
den layers differ from those associated with the
original template in 1-norm.
Figure 3 shows that as aDANgets deeper, the dif-
ferences between negative and positive sentences
become increasingly amplified. While nonexistent
in the shallowNBOWmodel, these differences are
visible even with just a single hidden layer, thus
explaining why deepening theNBOWimproves sen-
timent analysis as shown in Figure 4.
5.2    Handling Negations and "but": Where
Syntax is Still Needed
WhileDANs outperform other bag-of-words mod-
els, how can they model linguistic phenomena such
as negation without considering word order?  To
evaluateDANs over tougher inputs, we collect 92
sentences, each of which contains at least one nega-
tion and one contrastive conjunction, from the dev
and test sets of theSST.
9
Our fine-grained accuracy
ishigheron this subset than on the full dataset,
improving almost five percent absolute accuracy
to 53.3%. TheDRecNNmodel of
 ̇
Irsoy and Cardie
(2014) obtains a similar accuracy of 51.1%, con-
trary to our intuition that syntactic functions should
outperform unordered functions on sentences that
clearly require syntax to understand.
10
Are these sentences truly difficult to classify? A
close inspection reveals that both theDANand the
DRecNNhave an overwhelming tendency to pre-
dict negative sentiment (60.9% and 55.4% of the
time for theDANandDRecNNrespectively) when
they see a negation compared to positive sentiment
(35.9% forDANs, 34.8% forDRecNNs). If we fur-
ther restrict our subset of sentences to only those
with positive ground truth labels, we find that while
both models struggle, theDRecNNobtains 41.7%
accuracy, outperforming theDAN's 37.5%.
To understand why a negation or contrastive con-
junction triggers a negative sentiment prediction,
9
We search for non-neutral sentences containingnot/n't,
andbut. 48 of the sentences are positive while 44 are negative.
10
Both models are initialized with pretrained 300-d GloVe
embeddings for fair comparison.

SentenceDANDRecNNGround Truth
alousymovie  that'snotmerelyunwatchable,   but  also
unlistenable
negativenegativenegative
if  you're
not
a
prepubescentgirl
,  you'll  be
laughing
at
britneyspears'movie-starringdebutwhenever it doesn't
have you
impatientlysquinting
at yourwatch
negativenegativenegative
blessed
with
immensephysicalprowess
he
may
well be, but
aholaissimplynotanactor
positiveneutralnegative
who
knows
whatexactlygodardis on about in this
film
, but
hiswordsand images don'thave toaddup tomesmerize
you.
positivepositivepositive
it's sogoodthat itsrelentless,polishedwitcanwithstand
notonlyineptschoolproductions, but evenoliverparker's
movieadaptation
negativepositivepositive
toobad
, but
thanks
to some
lovelycomedicmoments
and
severalfineperformances, it'snotatotalloss
negativenegativepositive
this movie wasnotgoodnegativenegativenegative
this movie was
goodpositivepositive
positive
this movie wasbadnegativenegativenegative
the movie wasnotbadnegativenegativepositive
Table 3:  Predictions ofDANandDRecNNmodels on real (top) and synthetic (bottom) sentences that
contain negations and contrastive conjunctions. In the first column, words colored red individually predict
the negative label when fed to aDAN, while blue words predict positive. TheDANlearns that the negators
notandn'tare strong negative predictors, which means it is unable to capture double negation as in the
last real example and the last synthetic example. TheDRecNNdoes slightly better on the synthetic double
negation, predicting a lower negative polarity.
we show six sentences from the negation subset and
four synthetic sentences in Table 3, along with both
models' predictions. The token-level predictions in
the table (shown as colored boxes) are computed by
passing each token through theDANas separate test
instances. The tokensnotandn'tare strongly pre-
dictive of negative sentiment. While this simplified
"negation" works for many sentences in the datasets
we consider, it prevents theDANfrom reasoning
about double negatives, as in "this movie was not
bad". TheDRecNNdoes slightly better in this case
by predicting a lesser negative polarity than the
DAN; however, we theorize that still more powerful
syntactic composition functions (and more labelled
instances of negation and related phenomena) are
necessary to truly solve this problem.
5.3    Unsupervised Embeddings Capture
Sentiment
Our model consistently converges slower to a worse
solution  (dropping  3%  in  absolute  accuracy  on
coarse-grainedSST) when we randomly initialize
the word embeddings. This does not apply to just
DANs; both convolutional and recursive networks
do the same (Kim, 2014;
 ̇
Irsoy and Cardie, 2014).
Why are initializations with these embeddings so
crucial to obtaining good performance?  Is it pos-
sible that unsupervised training algorithms are al-
ready capturing sentiment?
We investigate this theory by conducting a sim-
ple experiment: given a sentiment lexicon contain-
ing both positive and negative words, we train a
logistic regression to discriminate between the asso-
ciated word embeddings (without any fine-tuning).
We use the lexicon created by Hu and Liu (2004),
which consists of 2,006 positive words and 4,783
negative words.  We balance and split the dataset
into  3,000  training  words  and  1,000  test  words.
Using 300-dimensionalGloVeembeddings pre-
trained over the Common Crawl, we obtain over
95% accuracy on the unseen test set, supporting the
hypothesis that unsupervised pretraining over large
corpora can capture properties such as sentiment.
Intuitively, after the embeddings are fine-tuned
duringDANtraining, we might expect a decrease
in the norms of stopwords and an increase in the

norms of sentiment-rich words like "awesome" or
"horrible".   However,  we find no significant dif-
ferences between theL
2
norms of stopwords and
words  in  the  sentiment  lexicon  of  Hu  and  Liu
(2004).
6    Related Work
OurDANmodel builds on the successes of both
simple vector operations and neural network-based
models for compositionality.
There are a variety of element-wise vector op-
erations that could replace the average used in the
DAN. Mitchell and Lapata (2008) experiment with
many of them to model the compositionality of
short phrases.  Later, their work was extended to
take into account the syntactic relation between
words (Erk and Pad
 ́
o, 2008; Baroni and Zampar-
elli, 2010;  Kartsaklis and Sadrzadeh, 2013) and
grammars (Coecke et al., 2010; Grefenstette and
Sadrzadeh, 2011). While the average works best for
the tasks that we consider, Banea et al. (2014) find
that simply summingword2vecembeddings out-
performs all other methods on the SemEval 2014
phrase-to-word and sentence-to-phrase similarity
tasks.
Once we compute the embedding average in a
DAN, we feed it to a deep neural network. In con-
trast, most previous work on neural network-based
methods forNLPtasks explicitly model word or-
der.  Outside of sentiment analysis,RecNN-based
approaches  have  been  successful  for  tasks  such
as parsing (Socher et al., 2013a), machine trans-
lation  (Liu  et  al.,  2014),  and  paraphrase  detec-
tion  (Socher  et  al.,  2011a).   Convolutional  net-
works also model word order in local windows and
have achieved performance comparable to or bet-
ter than that ofRecNNs on many tasks (Collobert
and Weston, 2008; Kim, 2014). Meanwhile, feed-
forward  architectures  like  that  of  theDANhave
been used for language modeling (Bengio et al.,
2003), selectional preference acquisition (Van de
Cruys, 2014), and dependency parsing (Chen and
Manning, 2014).
7    Future Work
In Section 5, we showed that the performance of
ourDANmodel  worsens  on  sentences  that  con-
tain lingustic phenomena such as double negation.
One promising future direction is to cascade clas-
sifiers  such  that  syntactic  models  are  used  only
when aDANis not confident in its prediction. We
can also extend theDAN's success at incorporating
out-of-domain training data to sentiment analysis:
imagine training aDANon labeled tweets for clas-
sification on newspaper reviews.  Another poten-
tially interesting application is to add gated units
to aDAN,as has been done for recurrent and recur-
sive neural networks (Hochreiter and Schmidhuber,
1997; Cho et al., 2014; Sutskever et al., 2014; Tai
et  al.,  2015),  to  drop  useless  words  rather  than
randomly-selected ones.
8    Conclusion
In this paper, we introduce the deep averaging net-
work, which feeds an unweighted average of word
vectors through multiple hidden layers before clas-
sification.  TheDANperforms competitively with
more complicated neural networks that explicitly
model semantic and syntactic compositionality. It
is further strengthened by word dropout, a regu-
larizer that reduces input redundancy.DANs ob-
tain close to state-of-the-art accuracy on both sen-
tence and document-level sentiment analysis and
factoid question-answering tasks with much less
training time than competing methods; in fact, all
experiments were performed in a matter of min-
utes on a single laptop core.   We find that both
DANs and syntactic functions make similar errors
given syntactically-complex input, which motivates
research into more powerful models of composi-
tionality.
Acknowledgments
We thank Ozan
 ̇
Irsoy not only for many insight-
ful  discussions  but  also  for  suggesting  some  of
the  experiments  that  we  included  in  the  paper.
We also thank the anonymous reviewers, Richard
Socher,  Arafat  Sultan,  and  the  members  of  the
UMD "Thinking on Your Feet" research group for
their helpful comments. This work was supported
byNSFGrant IIS-1320538.  Boyd-Graber is also
supported byNSFGrantsCCF-1409287 andNCSE-
1422492. Any opinions, findings, conclusions, or
recommendations expressed here are those of the
authors and do not necessarily reflect the view of
the sponsor.

References
Carmen Banea, Di Chen, Rada Mihalcea, Claire Cardie, and
Janyce Wiebe.   2014.   Simcompass:  Using deep learn-
ing word embeddings to assess cross-level similarity.  In
SemEval.
Marco Baroni and Roberto Zamparelli.   2010.   Nouns are
vectors, adjectives are matrices:  Representing adjective-
noun constructions in semantic space.  InProceedings of
Empirical Methods in Natural Language Processing.
Yoshua Bengio, R
 ́
ejean Ducharme, Pascal Vincent, and Chris-
tian Jauvin. 2003. A neural probabilistic language model.
Journal of Machine Learning Research.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2013.
Representation learning: A review and new perspectives.
IEEE Transactions on Pattern Analysis and Machine Intel-
ligence, 35(8):1798–1828.
Jordan Boyd-Graber, Brianna Satinoff, He He, and Hal Daum
 ́
e
III. 2012. Besting the quiz master: Crowdsourcing incre-
mental classification games. InProceedings of Empirical
Methods in Natural Language Processing.
Danqi Chen and Christopher D Manning.  2014.  A fast and
accurate  dependency  parser  using  neural  networks.   In
Proceedings of Empirical Methods in Natural Language
Processing.
Kyunghyun  Cho,  Bart  van  Merrienboer,  Caglar  Gulcehre,
Fethi  Bougares,  Holger  Schwenk,  and  Yoshua  Bengio.
2014. Learning phrase representations using rnn encoder-
decoder for statistical machine translation. InProceedings
of Empirical Methods in Natural Language Processing.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark. 2010.
Mathematical  foundations  for  a  compositional  distribu-
tional model of meaning.Linguistic Analysis (Lambek
Festschirft).
Ronan  Collobert  and  Jason  Weston.   2008.   A  unified  ar-
chitecture for natural language processing:  Deep neural
networks with multitask learning.  InProceedings of the
International Conference of Machine Learning.
George E Dahl, Ryan P Adams, and Hugo Larochelle. 2012.
Training restricted boltzmann machines on word observa-
tions.  InProceedings of the International Conference of
Machine Learning.
John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive
subgradient  methods  for  online  learning  and  stochastic
optimization.Journal of Machine Learning Research.
Katrin Erk and Sebastian Pad
 ́
o.  2008.  A structured vector
space model for word meaning in context. InProceedings
of Empirical Methods in Natural Language Processing.
Edward Grefenstette and Mehrnoosh Sadrzadeh.  2011.  Ex-
perimental support for a categorical compositional distri-
butional model of meaning. InProceedings of Empirical
Methods in Natural Language Processing.
Karl Moritz Hermann, Edward Grefenstette, and Phil Blun-
som.  2013.  "not not bad" is not "bad":  A distributional
account of negation.Proceedings of the ACL Workshop on
Continuous Vector Space Models and their Compositional-
ity.
Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya
Sutskever, and Ruslan Salakhutdinov.  2012.  Improving
neural networks by preventing co-adaptation of feature
detectors.CoRR, abs/1207.0580.
Sepp Hochreiter and J
 ̈
urgen Schmidhuber. 1997. Long short-
term memory.Neural computation.
Minqing Hu and Bing Liu.   2004.   Mining and summariz-
ing customer reviews. InKnowledge Discovery and Data
Mining.
Ozan
 ̇
Irsoy and Claire Cardie.  2014.  Deep recursive neural
networks for compositionality in language. InProceedings
of Advances in Neural Information Processing Systems.
Mohit   Iyyer,   Jordan   Boyd-Graber,   Leonardo   Claudino,
Richard Socher,  and Hal Daum
 ́
e III.   2014a.   A neural
network for factoid question answering over paragraphs.
InProceedings of Empirical Methods in Natural Language
Processing.
Mohit  Iyyer,  Peter  Enns,  Jordan  Boyd-Graber,  and  Philip
Resnik.  2014b.  Political ideology detection using recursive
neural networks.   InProceedings of the Association for
Computational Linguistics.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent convo-
lutional neural networks for discourse compositionality. In
ACL Workshop on Continuous Vector Space Models and
their Compositionality.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom.
2014. A convolutional neural network for modelling sen-
tences.  InProceedings of the Association for Computa-
tional Linguistics.
Dimitri Kartsaklis and Mehrnoosh Sadrzadeh.  2013.  Prior
disambiguation of word tensors for constructing sentence
vectors. InProceedings of Empirical Methods in Natural
Language Processing.
Yoon Kim. 2014. Convolutional neural networks for sentence
classification.   InProceedings of Empirical Methods in
Natural Language Processing.
Quoc V Le and Tomas Mikolov. 2014. Distributed represen-
tations of sentences and documents. InProceedings of the
International Conference of Machine Learning.
Jiwei Li.  2014.  Feature weight tuning for recursive neural
networks.CoRR, abs/1412.3714.
Shujie Liu,  Nan Yang,  Mu Li,  and Ming Zhou.   2014.   A
recursive recurrent neural network for statistical machine
translation. InProceedings of the Association for Compu-
tational Linguistics.
Andrew  L.  Maas,  Raymond  E.  Daly,  Peter  T.  Pham,  Dan
Huang, Andrew Y. Ng, and Christopher Potts.  2011.  Learn-
ing word vectors for sentiment analysis. InProceedings of
the Association for Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based models
of semantic composition.  InProceedings of the Association
for Computational Linguistics.
Bo Pang and Lillian Lee.   2005.   Seeing stars:  Exploiting
class relationships for sentiment categorization with respect
to rating scales.   InProceedings of the Association for
Computational Linguistics.

Jeffrey Pennington, Richard Socher, and Christopher Manning.
2014.  Glove: Global vectors for word representation.  In
Proceedings of Empirical Methods in Natural Language
Processing.
Asad B. Sayeed, Jordan Boyd-Graber, Bryan Rusk, and Amy
Weinberg.  2012.  Grammatical structures for word-level
sentiment detection.   InNorth American Association of
Computational Linguistics.
Richard Socher, Eric H. Huang, Jeffrey Pennington, Andrew Y.
Ng, and Christopher D. Manning. 2011a. Dynamic Pool-
ing and Unfolding Recursive Autoencoders for Paraphrase
Detection. InProceedings of Advances in Neural Informa-
tion Processing Systems.
Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y.
Ng, and Christopher D. Manning.  2011b.  Semi-Supervised
Recursive Autoencoders for Predicting Sentiment Distri-
butions. InProceedings of Empirical Methods in Natural
Language Processing.
Richard Socher, John Bauer, Christopher D. Manning, and
Andrew Y. Ng. 2013a. Parsing With Compositional Vector
Grammars. InProceedings of the Association for Compu-
tational Linguistics.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang,
Christopher D Manning, Andrew Y Ng, and Christopher
Potts.  2013b.  Recursive deep models for semantic com-
positionality over a sentiment treebank. InProceedings of
Empirical Methods in Natural Language Processing.
Nitish Srivastava,  Geoffrey Hinton,  Alex Krizhevsky,  Ilya
Sutskever, and Ruslan Salakhutdinov.  2014.  Dropout: A
simple way to prevent neural networks from overfitting.
Journal of Machine Learning Research, 15(1).
Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.  2014.  Se-
quence  to  sequence  learning  with  neural  networks.   In
Proceedings of Advances in Neural Information Processing
Systems.
Kai  Sheng  Tai,  Richard  Socher,  and  Christopher  D.  Man-
ning. 2015. Improved semantic representations from tree-
structured long short-term memory networks.
Tim Van de Cruys. 2014. A neural network approach to selec-
tional preference acquisition. InProceedings of Empirical
Methods in Natural Language Processing.
Sida I. Wang and Christopher D. Manning. 2012. Baselines
and bigrams: Simple, good sentiment and topic classifica-
tion. InProceedings of the Association for Computational
Linguistics.
Jason Weston,  Samy Bengio,  and Nicolas Usunier.   2011.
Wsabie: Scaling up to large vocabulary image annotation.
InInternational Joint Conference on Artificial Intelligence.
